%!tex root=./thesis.tex
\chapter{Introduction}
\label{cha:intro}

% \begin{quote}
%     There exist problems in astronomy that you cannot solve by downloading XGB and hitting them with a hammer. This is why we need to do astroinformatics research.

%    ---Cheng Soon Ong
% \end{quote}

Many great results come from study at the intersection of two fields, and the combination of astronomy and informatics is no exception. The resulting interdisciplinary field is called \defn{astroinformatics}, and concerns the application of statistical and machine learning techniques to problems in astronomy and astrophysics.
% I am a strong believer in the benefits of interdisciplinary study to both fields: by studying astroinformatics, we can gain new applications for existing machine learning methodology, as well as gain a greater understanding of the problems faced in astronomy.
Machine learning, a collection of methods for formalising and solving data-driven problems at scale, is a natural fit for radio astronomy: Radio astronomy faces a `data deluge' thanks to new and upgraded telescopes and associated wide-area surveys to be undertaken with them. The goal in the near future is to be able to process data---and conduct science on that data---on the fly as the volume of data grows too large to store. This is a requirement for the Square Kilometre Array (SKA), a grand international undertaking to build a distributed radio array across Western Australia and South Africa with a collecting area of one square kilometre. There exists no comparable telescope today. The SKA will be able to perform new tests of general relativity, help pin down the large-scale structure of the Universe, investigate the mysteries of the epoch of reionisation, probe the history of galaxy evolution to new extents, and perhaps make unexpected new discoveries \citep{diamond_ska_2017}.

The SKA will have technical challenges thus far only touched upon by radio astronomy. Raw data will stream from the telescope antennae at 2 petabytes per second, and up to 300 petabytes per year of science data is expected to be generated \citep{diamond_ska_2017}. This is a phenomenal amount of data, much of which won't be stored, and the community expectation is that machine learning will provide avenues to conduct science with such a large dataset. Precursor projects to the SKA have begun to investigate these pathways \citep[e.g.][]{kapinska_emu_2020,mostert_unveiling_2020}.

Even without the SKA, the data deluge has already begun. Three precursor telescopes have been constructed: the Australian Square Kilometre Array Pathfinder (ASKAP) and Murchison Widefield Array (MWA) in Western Australia, and MeerKAT in South Africa. While these will eventually form part of the SKA, they are already online and generating science data at astonishing rates. ASKAP will soon conduct six surveys of the entire southern radio sky: the Evolutionary Map of the Universe \citep[EMU;][]{norris11,kapinska_emu_2020}, the Widefield ASKAP L-Band Legacy All-Sky Blind Survey \citep[WALLABY]{koribalski_wallaby_2020}, the First Large Absorption Survey in HI (FLASH), an ASKAP Survey for Variables and Slow Transients \citep[VAST;][]{murphy_vast_2013}, the Galactic ASKAP Spectral Line Survey (GASKAP), and the Polarisation Sky Survey of the Universe's Magnetism (POSSUM). Atop these future surveys, the recent Rapid ASKAP Continuum Survey \citep[RACS;][]{mcconnell_rapid_2020} has redefined our knowledge of the southern radio sky with shallow observations at 15 arcsecond resolution---compare to the previous largest radio survey, the NRAO VLA Sky Survey \citep[NVSS;][]{condon98nvss}, with 45 arcsecond resolution over the northern sky.

Machine learning methods for radio astronomy will be developed for and tested upon surveys like RACS and EMU. The path between an astronomical problem and a machine learning problem, however, is not a straightforward one. The goal when casting an astronomy question as something mathematical or computational is to convert the question into one with a known method of solution, such as classification or regression. Along the way, astronomical concepts and assumptions need to be turned into something a computer can deal with. Despite the wide availability of machine learning software and tools, there is no automatic or easy way to make this transformation. This necessitates research in astroinformatics.

This thesis concerns applications of machine learning to radio astronomy for the identification of extended extragalactic radio sources in wide-area surveys. I will present new methods of cross-identifying radio objects with their corresponding infrared and optical observations, demonstrate the applicability of these methods to existing wide-area radio surveys, and develop a new way to identify complexity in polarised radio sources.


\section{Problems in extragalactic radio astronomy}
\label{sec:problems-radio-astronomy}

    The main objects of interest in extragalactic radio astronomy are active galactic nuclei (AGN, \autoref{sec:agn}), intensely energetic objects at the centre of galaxies. AGN actively accrete matter and eject huge jets of plasma that develop into extended lobes over scales much larger than those of the galaxy itself. Radio astronomy has many uses for AGN: their energy scales provide a test-bed for high energy physics, and the extremely bright lobes and jets can be seen throughout the Universe making AGN an accessible probe of the distant and old Universe.

    AGN are thought to be critical to galaxy evolution and perhaps the early reionisation of the Universe \citep{bosch-ramon_role_2018}, but their exact role in their host galaxies is an open question. The radiative and mechanical energy released by AGN impacts the interstellar medium (ISM) and is a key component of contemporary galaxy simulations and models \citep{morganti_many_2017}. The quenching of star formation due to AGN activity is called \defn{AGN feedback}, the idea being that the energy expelled from an AGN is returned to the ISM of the galaxy, heating the gas so it cannot condense into stars. The different impacts of radiative and mechanical energy, the location and scales within the galaxy for which star formation is quenched, and whether star formation material is ejected from the galaxy by the AGN are all questions \citep{husemann_reality_2018}. Solving these questions requires an understanding of AGN at both small and large scales, as well as connecting AGN to their host galaxies at other wavelengths so that redshift, emission lines, star formation rates, etc. can be determined.

    The large scales of AGN also provide insight into the larger-scale structure of the Universe. Giant radio galaxies for example (\autoref{sec:what-we-see-agn}) are difficult to identify due to their size and disconnected appearance (\autoref{sec:aggregation}), but are so large that they can be used to probe galaxy clusters \citep{banfield_radio_2016} and even the large-scale structure of the Universe \citep{reiprich_abell_2020}. Other large-scale effects seem to exist, such as the apparent alignment of radio galaxies \citep{contigiani_radio_2017,panwar_alignment_2020}, though investigation continues as to whether this effect is real or due to some unknown systematic bias. New radio surveys will reveal more radio sources than ever before, and if they can be identified, radio structures in these surveys will allow us to investigate the structure of the Universe.

    The magnetic structure of AGN and their extended lobes may be probed by radio polarimetry observations \citep{anderson_broadband_2015,grant11polarised}. Through polarisation, though, extragalactic AGN can provide insight into our own Galaxy: the Faraday depth and complexity (\autoref{sec:polarisation}) of extragalactic radio sources can be used to quantify local magnetic fields. With more polarised radio sources to be revealed through upcoming wide-area polarisation surveys, the magnetic field of the Milky Way can be better resolved.

    Other problems in radio astronomy relate to the new level of data that we are about to obtain from large telescopes like the SKA. Methods for dealing with radio data at scale are still very much in their infancy, and need to be developed before instruments like ASKAP and MeerKAT can be used to their full potential. Much of this data processing will have to be `online', meaning that data will need to be processed as they are generated (and likely these data will not be stored at all). This is in contrast to how most science results are currently found, with new discoveries coming from legacy surveys many years after they were conducted. Some discoveries are spontaneous, and losing the ability to make serendipitous discoveries would be a major blow to astronomy \citep{norris17unexpected}: how can we deal with so much data but still retain the ability to discover the unknown?

\section{Big data in astronomy}
\label{sec:big-data}

    The scale of radio data underpins many of the methodology problems facing radio astronomy. There are two main scientific benefits that come from large-scale data: better statistics, and more unusual objects.

    Many results in astronomy are statistical, from measuring the expansion of the Universe to understanding the distribution of galaxy properties. With more observations we can not just narrow the uncertainty of these results, but also diversify them. When the number of objects under study is large, we can subdivide the population into subpopulations based on their physical properties and determine a statistic on each subpopulation. This can help understand the physical basis behind the statistic, or 
    remove unwanted subpopulations from analysis. Even with less data it is still possible to subdivide or filter populations, but this will dramatically raise the uncertainty in the results due to the low sample size of each bin. An example of such a statistic is the radio luminosity function (RLF), which describes the density of radio sources throughout the Universe. It can be divided into a fractional RLF to examine the distribution of subpopulations, or to remove the effect of star-forming galaxies. I use large datasets to improve the uncertainty of RLFs and subdivide RLFs by the infrared properties of the population in \autoref{cha:rlfs}.

    With large datasets, highly unusual or rare objects are more likely to be included. Much of astronomy has been pushed forward by serendipitous discoveries, and (provided we have some way of combing through the dataset) large datasets should provide a wealth of such discoveries to be found. These may be found either through identifying objects where statistical methods seem to fail, or perhaps through direct searches \citep{norris17unexpected}.

    But with the benefits of big data come new challenges. At these new scales, the ability to store all of our science data is no longer a given. Many methods that previously had the luxury to run over a whole dataset at their own pace will now need to process data on-the-fly. Even with storage, the scale is tremendous: FIRST, for example, contains around 900~000 sources, of which very few were manually labelled. Over 10~000 volunteers labelled interesting objects in FIRST over four years of the Radio Galaxy Zoo project, with 75~000 aggregated labels passing quality assurance testing. While a phenomenal and, in radio, unparalleled labelling effort, it pales in comparison to the estimated 70~000~000 sources that EMU will find \citep{banfield15}.

\section{Machine learning in astronomy}
\label{sec:ml-in-astro}

    why machine learning? As distinct from a computer program that applies rules. \begin{itemize}
        \item we don't know what rules to apply
        \item we are pretty good at doing these things by hand
        \item we have lots of data
    \end{itemize}
    why is astronomy a good application of machine learning? E.g. As distinct from computer vision for self driving cars. \begin{itemize}
        \item different noise properties to natural images e.g. radio has correlated noise
        \item 
    \end{itemize}

\section{How this thesis fits in}
\label{sec:how-this-fits}
    Why these two particular problems, and how they relate to the major questions in astronomy. \begin{itemize}
        \item big issues facing radio astronomy and without solving them we won't be able to make full use of the SKA
        \item cross-identification is critical to determining galaxy properties (nothing so boring as a radio source)
        \item lots of magnetic field questions require knowing whether we are looking at reliable bulk sources or not
    \end{itemize}


\section{Thesis Outline}
\label{sec:outline}

We begin by introducing key concepts from radio astronomy in \autoref{cha:background}, including radio active galactic nuclei (AGN), as well as motivations for and difficulties in cross-identifying observed radio emission from AGN. \autoref{cha:background-ml} introduces machine learning and describes the machine learning background required for the remainder of the thesis. These chapters together comprise the introduction.

\autoref{cha:cross-id} is my paper \emph{Radio Galaxy Zoo: Machine learning for radio source host galaxy cross-identification}. This chapter describes a new, machine learning method for cross-identifying extended radio emission with host galaxies in the infrared. We apply this approach to cross-identifying all extended radio sources in the Faint Images of the Radio Sky at Twenty Centimeters radio survey \citep[FIRST;][]{becker95first} with their infrared counterparts in All\emph{WISE} \citep{cutri2013wiseexplanatory} and use the resulting catalogue of cross-identifications to create a fractional radio luminosity function in \autoref{cha:rlfs}, which itself is my paper \emph{Radio Galaxy Zoo: Radio luminosity functions of extended sources}. \autoref{cha:faraday} is my paper \emph{Interpretable Faraday Complexity Classification}, which introduces an interpretable method for classifying radio emission as Faraday complex or simple, which could be used to identify whether it is an extended source that is below the resolution limit.

\section{Contributions}
\label{sec:contributions}

My main contributions to radio astroinformatics in this thesis are:
\begin{itemize}
    \item I introduce a new method for cross-identifying radio emission which can learn from existing catalogues, which is the first application of machine learning to cross-identification;
    \item I demonstrate an application of this new method to the creation of fractional radio luminosity functions, which require considerably more cross-identifications than non-fractional radio luminosity functions, in the process creating the largest available catalogue of extended, cross-identified radio sources;
    \item I produce a fractional radio luminosity function with divisions based on mid-infrared colours associated with the host galaxies of the radio emission, helping to understand how radio galaxies evolve throughout the Universe;
    \item I highlight the requirement of considerably more redshifts in understanding future wide-area radio surveys; and
    \item I introduce a new method to identify Faraday complexity using an interpretable classifier, as well as features for Faraday dispersion functions which can be used for other machine learning tasks.
\end{itemize}
