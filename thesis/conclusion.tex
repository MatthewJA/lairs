%!tex engine=xelatex
%!tex root=./thesis.tex
\chapter{Conclusion}
\label{cha:conc}

% This chapter needs to
% \begin{itemize}
%     \item summarise the thesis
%     \item draw together works
%     \item link each chapter to theory and methodology
%     \item evaluate the contribution of this work to the field
%     \item discuss future work
% \end{itemize}

The future of radio astronomy is immensely exciting, with upcoming radio surveys like EMU and POSSUM sure to revolutionise our understanding of the violent Universe occupied by radio active galactic nuclei. In this thesis we have explored methods for identifying extragalactic radio sources in these future wide-area radio surveys.

\autoref{cha:cross-id} developed a new approach to cross-identifying radio components with their infrared host galaxies and applied this method to the CDFS and ELAIS-S1 ATLAS survey, a pilot survey for the upcoming ASKAP key survey EMU. This was the first ever application of machine learning to radio cross-identification. In the process, we demonstrated that our methods worked on ELAIS-S1 with similar performance to how they worked on CDFS, despite these being different parts of the sky. We also evaluated simple nearest-neighbour cross-identification on the ATLAS fields, showing better performance than either machine learning or Radio Galaxy Zoo volunteers on this dataset, which we suggest is because of how most objects in ATLAS are very compact. Our machine learning methods performed similarly whether they were trained on expert-provided or citizen science-provided labelled datasets, clearly demonstrating the benefit of citizen science labels to radio astroinformatics even if they may be less accurate than labels provided by expert. We showed that the algorithm we had developed could in principle perform much better than nearest-neighbours if only its binary accuracy could be raised, perhaps with a more complex or better-trained classifier. Finally, we concluded that ATLAS was an insufficiently complex dataset to train machine learning algorithms for future radio surveys: more complex and larger training sets would be necessary.

To further investigate our classification-based cross-identification algorithm we needed a more complex dataset, and in \autoref{cha:rlfs} we turned to FIRST, a legacy survey containing around 900~000 radio components. Of these, 250~000 or so were not compact, compared to the vast majority of ATLAS. We increased the complexity of our machine learning model and trained it on this dataset along with 75~000 Radio Galaxy Zoo labels, far more than we had available for ATLAS. The resulting classifier could then be used to cross-identify every extended object in FIRST. Using the fact that any two radio components in the same radio source should also have the same host galaxy, we identified not only the host galaxies of our radio components but also their association to other radio components. This allowed us to produce a catalogue of over 150~000 extended radio sources, the largest extant catalogue of extended radio sources at the time of writing. Such a large catalogue enabled us to estimate a RLF for extended radio sources, the first time a RLF has been produced for just extended sources. We could also subdivide this RLF into a fractional RLF, and we were able to produce RLFs divided by the mid-infrared colour of the host galaxy, their physical extent, and their redshift. Extended radio sources ought to be the sources which contribute the most mechanical energy to their surrounding IGM, and with an RLF dedicated solely to extended sources we were able to estimate this energy contribution as $1.3 \times 10^{30}$ and $1.2 \times 10^{32}$~W~Mpc$^{-3}$. Perhaps most importantly, we showed our method works and used it to obtain a physically meaningful result. Our method can easily be extended to new surveys, as long as training data exist.

In \autoref{cha:faraday} we developed an interpretable Faraday complexity classifier. We constructed features that were easy to understand by measuring the distance of observations from a simple model of Faraday simple sources. Our resulting features could be calculated from both simulated and real observations. We demonstrated the effectiveness of our method on both simulated and real data and showed that on simulated data our simple methods matched the state-of-the-art CNN classifier despite having far less parameters. This was the first application of machine learning to real spectropolarimetric data. This method will be applicable to future surveys like POSSUM.

\section{Implications for radio citizen science}
\label{sec:implications-citizen-science}

    Our results in \autoref{cha:cross-id} demonstrated that machine learning methods trained on citizen science labels perform comparably to those trained on expert labels, even when those labels are lower quality than expert labels. We applied this insight in \autoref{cha:rlfs} to obtain scientific results, using machine learning to extrapolate the labels to a larger dataset. While Radio Galaxy Zoo alone was not enough to fully label FIRST, Radio Galaxy Zoo working in tandem with machine learning was. This is a pattern that may hold true for future surveys and applications, too. Future astronomical research at-scale may leverage the idea of people working with machine learning, sometimes called \defn{human-in-the-loop} \citep[e.g.][]{holzinger_interactive_2016} learning, to pore through data-at-scale.

\section{Implications for wide-area radio surveys}
\label{sec:implications-wide-area-radio-surveys}

    As we move toward larger and larger datasets, an important question is how applicable our models will be across the sky. Our results in \autoref{cha:cross-id} showed that we can expect some generalisation, as our model trained on one patch of sky was applicable to another without a great loss of performance. Similarly, our classifier trained on part of FIRST seemed to work well on the rest of the dataset. This is promising as it implies that limited area surveys may help develop training sets that generalise to the whole sky, potentially making the process of generating training sets considerably cheaper.

    Between \autoref{cha:cross-id} and \autoref{cha:rlfs} we demonstrated that a large set of good quality, complex data is required for training good astroinformatics algorithms. Pilot datasets like ATLAS will not work by themselves: the sources they contain are too simple and their complex sources are too few. A sensible question to ask is, could we simulate data for training purposes? We trained our classifier in \autoref{cha:faraday} on simulated data and found that it was difficult to bridge the domain gap between simulation and observation, even in a well-understood, one-dimensional case---let alone the complex three-dimensional projected morphologies we observe in imagery. Getting across this domain gap will be difficult and will necessarily be a major topic of research in the astroinformatics field in the near future.

    Tying observations, simulations, and models together are the representation of the data: features. Our results in \autoref{cha:faraday} show that judicious choice of features can outperform even complex and powerful models. This is good for two reasons. The first reason is that these features may be more easily interpreted. The meaning of the features may be understood to be representative of some physical property, or at least the relationship between physical reality and predictions may be more easily found. The second reason is that features may be selected which can generalise well to datasets beyond just the training set. In other words, features that are less overfit to the training set. This is of particular concern when developing predictive models on simulated training data, as features being less suited for real data than for simulations is one aspect that may contribute to the domain gap. Choosing good features in astronomy may be more important than in many other fields for which machine learning is applied, as while in most fields it is possible to conduct experiments, in astronomy we only have one Universe to look at. We need to make the best use we can of the limited radio sky.

% \section{What we need to move forward}
% \label{sec:moving-forward}
    
%     What does radio astronomy need to be able to press ahead with astroinformatics? From my work what do we need to do to set ourselves up for a solid future field?

    % \begin{itemize}
    %     \item better training data
    %     \item more redshifts
    % \end{itemize}


\section{Future Work}
\label{sec:future}

    There are many ways that the research in this thesis can be extended in future. We will summarise some of these here.

    Our methods can be extended in a number of ways. These fall into two categories: further applications and extensions to the algorithms. An obvious target for future work is to apply our cross-identification algorithm to new and upcoming surveys like LoTSS and EMU. These promise a tremendous amount of data with new discoveries certain to be lying in wait within, and cross-identifying the radio emission with its corresponding infrared or optical host galaxy will be vital for uncovering those secrets. Similarly we would like to apply our Faraday complexity classifier to future spectropolarimetric surveys like POSSUM. Our methodology can also be improved. Better models almost certainly exist than the CNN we applied to BXID. As we demonstrated in \autoref{cha:faraday}, a well thought-out model and features may best a complex model like the CNN. How would our cross-identification approach worked if, say, we applied it to hand-selected features such as those chosen by \citet{proctor06}? Would a search over more CNN architectures, like that described by \citet{lukic_morphological_2019}, result in better classifiers and hence better cross-identifications? Perhaps we could even improve performance by pre-training on some unlabelled but larger dataset? A less obvious improvement to our BXID approach would be to change how the classification scores are aggregated. Currently this is a weighted argmax over candidate host galaxies, but other methods are possible. The weights could be something other than a Gaussian function of distance, from other functions of distance to an entire separate classification model. Maybe we could aggregate the scores in bulk, using some kind of algorithm that assigns radio-host relationships based on not just the radio source itself but also on the other radio sources around it and how they have been paired to their own host galaxies.

    The way that our labels were generated for BXID could be improved. Our algorithms in \autoref{cha:cross-id} and \autoref{cha:rlfs} were trained on labels generated by Radio Galaxy Zoo. These labels were aggregated from multiple different labellers (usually 20) by majority vote, with the most common label for any give n radio object being assigned as the true label in Radio Galaxy Zoo. This is not the only possible aggregation strategy, though. We employed the Dawid-Skene method \citep{dawid79em} ourselves in \autoref{sec:rlfs-manual-validation} to help assess the performance of our cross-identification algorithm, and this model for example may also be applied to Radio Galaxy Zoo. There are in fact aggregation strategies that work in tandem with a machine learning model to get better labels, such as \citet{raykar_learning_2010}. These methods simultaneously take into account the labels and the labellers, and can accommodate for different levels of ability in the labellers, or different levels of difficulty in the examples being labelled.

    Our RLFs could be improved. The RLF calculations in \autoref{cha:rlfs} are severely limited by the availability of redshifts. We limit our analyses to host galaxies that do have available spectroscopic redshifts in SDSS, but we could also employ the less-reliable but considerably more prolific photometric redshifts. These are derived from regression models rather than direct observations of redshifted spectral lines, and so can be produced from photometric surveys without dedicated spectroscopy. However, without methods to handle the uncertainty introduced by photometric redshifts, the resulting RLFs would be unreliable. Developing ways to not only decrease the uncertainty in photometric redshifts but also to understand and incorporate the uncertainty into downstream calculations like those of the RLFs will allow these photometric redshifts to be used and tremendously increase sample sizes. This will be very important for surveys like EMU, which are both deep and wide with low redshift availability.

    The RGZ-Ex dataset (\autoref{cha:rlfs}) also lets us pose many other interesting science questions. We demonstrated in \autoref{sec:rlfs-giants} that rare galaxy classes can be identified from within this dataset, including examples that have never before been identified in the literature. Our dataset may be augmented with other features and used to identify unusual objects in a similar way. Besides this, our fractional RLFs could also be extended with any number of galaxy properties. One particularly interesting property could be morphology, as other algorithms in radio astroinformatics are developed which can automatically identify morphologies \citep[e.g.][]{wu19claran}: such a classifier could be used to segment RGZ-Ex and a fractional morphological RLF could be obtained. Of course, there are other properties that are more easily extracted, such as optical lines and colours which could be taken from SDSS using our SDSS cross-identifications.

     While creating features for FDFs in \autoref{cha:faraday} we demonstrated that W2 distance was a sensible distance measure between FDFs. This is useful for more than just feature construction, as it implies a geometry on the space of FDFs. This distance could be used to help gain insight on the behaviours of future algorithms that work on FDFs. A particularly exciting idea is to improve QU fitting by modifying the distance function to match ours. Our features could also be used to develop other methods for FDF analysis, like outlier detection or data visualisation.

     Further research is needed to close the domain gap for FDFs. This is an interesting case study as it is such a simple case, where we know essentially all the physics behind the observations and the observations are one-dimensional. Even this is not enough, and whether through unmodelled physics (e.g. more than two screens, depolarisation) or unmodelled observational properties (e.g. radio frequency interference) simulation and observation do not fully line up. This is critical if we want to train machine learning algorithms on simulations in the future, and we very much want to do this to augment our limited observational training data. Similarly the domain gap must be reduced for radio continuum imagery. Our results in \autoref{cha:cross-id} show that pilot surveys like ATLAS may not contain enough complex sources to train machine learning models, and while larger surveys like FIRST exist, transferring models from a survey undertaken with one set of observing parameters (telescope, frequency, depth, resolution...) is both non-trivial and as-yet relatively unexplored.

    % - domain gap for training on simulated data for FDFs
    % - domain gap for simulated radio continuum data to augment surveys like ATLAS
    % - apply our cross-identification and Faraday complexity methods to EMU and POSSUM, as well as surveys like LoTSS and GLEAM-X
    % - improve our aggregation method for bxid
    % - generate alternative fractional RLFs, e.g. by ML-derived morphology
    % - incorporate photometric redshifts into calculations from rlf
    % - investigate the link between extended sources and their local environments
    % - apply other crowd aggregation strategies to RGZ
    % - use our insights into FDFs to do other interesting things, like outlier detection, data visualisation, or some new way of qu fitting
