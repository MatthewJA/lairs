%!tex engine=xelatex
%!tex root=./thesis.tex
\chapter{Conclusion}
\label{cha:conc}

% This chapter needs to
% \begin{itemize}
%     \item summarise the thesis
%     \item draw together works
%     \item link each chapter to theory and methodology
%     \item evaluate the contribution of this work to the field
%     \item discuss future work
% \end{itemize}

The future of radio astronomy is immensely exciting, with upcoming radio surveys like EMU and POSSUM sure to revolutionise our understanding of the violent Universe occupied by radio active galactic nuclei. Without innovative new methods for processing astronomical data at scale, however, we will be limited in what interesting physics we can learn about with these surveys. In this thesis we have explored methods for identifying extragalactic radio sources in these future wide-area radio surveys. We developed a new automated cross-identification approach and a new way of classifying radio sources as Faraday complex or Faraday simple. We applied our methods to real data, and used our cross-identification method to directly probe the mechanical energy contribution of active galactic nuclei (AGN) to the intergalactic medium (IGM). Our contributions here are all aimed at extracting more information from the very large radio surveys that we have begun to face.

\autoref{cha:cross-id} developed a new approach to cross-identifying radio components with their infrared host galaxies and applied this method to the CDFS and ELAIS-S1 ATLAS survey, a pilot survey for the upcoming ASKAP key survey EMU. This was the first application of machine learning to radio cross-identification. In the process, we demonstrated that our methods worked on ELAIS-S1 with similar performance to how they worked on CDFS, despite these being different parts of the sky. We also evaluated simple positional matching cross-identification on the ATLAS fields, showing better performance than either machine learning or Radio Galaxy Zoo volunteers on this dataset, which we suggest is due to the compact nature of most objects in ATLAS. Our machine learning methods performed similarly whether they were trained on expert-provided or citizen science-provided labelled datasets, clearly demonstrating the benefit of citizen science labels to radio astroinformatics even if they may be less accurate than labels provided by experts. We showed that the algorithm we developed could in principle perform much better than positional matching if only its binary accuracy could be raised, perhaps with a more complex or better-trained classifier. Finally, we concluded that ATLAS was an insufficiently complex dataset to train machine learning algorithms for future radio surveys. ATLAS contains many compact sources, and while there will be many such sources in EMU and other future surveys, there will also be a zoo of partially resolved, extended, strange, and unusual morphologies. Methods designed or trained on a dataset like ATLAS, which lacks diversity of non-compact sources, will not work on the wide range of extended radio sources that will appear. More complex and larger training sets, perhaps real or perhaps simulated, will be necessary for producing algorithms intended for use in future surveys.

To further investigate our classification-based cross-identification algorithm we \linebreak needed a more complex dataset, and in \autoref{cha:rlfs} we turned to FIRST, a legacy survey containing around 900~000 radio components. Of these, 250~000 or so were not compact, compared to the vast majority of ATLAS. We increased the complexity of our machine learning model and trained it on this dataset along with 75~000 Radio Galaxy Zoo labels, far more than we had available for ATLAS. The resulting classifier could then be used to cross-identify every extended object in FIRST. Using the fact that any two radio components in the same radio source should also have the same host galaxy, we identified not only the host galaxies of our radio components but also their associations with other radio components. This allowed us to produce the RGZ-Ex catalogue, containing over 150~000 extended radio sources---the largest existing catalogue of extended radio sources at the time of writing. In this catalogue we identified 40 giant radio galaxies, most of which were new to literature.

Such a large catalogue enabled us to estimate a radio luminosity function (RLF) for extended radio sources, the first time a RLF has been produced for just extended sources. We could also subdivide this RLF into a fractional RLF, and we were able to produce RLFs divided by the mid-infrared colour of the host galaxy, physical extent, and redshift. Extended radio sources ought to be the sources which contribute the most mechanical energy to their surrounding IGM, and with an RLF dedicated solely to extended sources, we were able to estimate this energy contribution as $1.3 \times 10^{30}$ and $1.2 \times 10^{32}$~W~Mpc$^{-3}$. Perhaps most importantly, we showed that our cross-identification approach works and used it to obtain a physically meaningful result. Our methods can easily be extended to new surveys, as long as sufficient host galaxy/radio component pairs are known so that the algorithm can be trained.

Our work on cross-identification allowed us to extract more information from wide-area continuum radio surveys. Additionally, to help gain more use out of large polarisation surveys, we developed an interpretable Faraday complexity classifier for Faraday dispersion functions (FDFs) in \autoref{cha:faraday}. We constructed features that were easy to understand by measuring the distance of observations from a simple model of Faraday simple sources. Our resulting features could be calculated from both simulated and real observations. We demonstrated the effectiveness of our method on both simulated and real data and showed that on simulated data our simple methods matched the state-of-the-art convolutional neural network (CNN) classifier despite having far less parameters. This was the first application of machine learning to real spectropolarimetric data. We highlighted the domain gap between simulated and observed FDFs and suggested reframing the Faraday complexity classification task as finding simple and non-simple sources. This method will be applicable to future surveys like POSSUM.

% \section{What we need to move forward}
% \label{sec:moving-forward}
    
%     What does radio astronomy need to be able to press ahead with astroinformatics? From my work what do we need to do to set ourselves up for a solid future field?

    % \begin{itemize}
    %     \item better training data
    %     \item more redshifts
    % \end{itemize}


\section{Future work}
\label{sec:future}

    There are many ways that the research in this thesis can be extended in future. We will summarise some of these here.

    Our methods can be extended in a number of ways, which fall into two categories: further applications and extensions to the algorithms. An obvious target for future work is to apply our binary cross-identification algorithm (BXID; \autoref{sec:atlas-xid-as-binary-classification} and \autoref{sec:rlfs-method}) to new and upcoming surveys like LoTSS and EMU. These promise a tremendous amount of data with new discoveries certain to be lying in wait within, and cross-identifying the radio emission with its corresponding infrared or optical host galaxy will be vital for uncovering those secrets. Similarly, we would like to apply our Faraday complexity classifier to future spectropolarimetric surveys like POSSUM.

    Our methodology can also be improved. Better models almost certainly exist than the CNN we applied to BXID. As we demonstrated in \autoref{cha:faraday}, a well thought-out model and features may best a complex model like the CNN. How well would our cross-identification approach work if, say, we applied it to hand-selected features such as those chosen by \citet{proctor06}? Would a search over more CNN architectures, like that described by \citet{lukic_morphological_2019}, result in better classifiers and hence better cross-identifications? Perhaps we could even improve performance by pre-training on some unlabelled but larger dataset? A less obvious improvement to our BXID approach would be to change how the classification scores are aggregated. Currently this is a weighted maximisation over candidate host galaxies, but other methods are possible. The weights could be something other than a Gaussian function of distance, from other functions of distance to an entire separate classification model. Maybe we could aggregate the scores in bulk, using some kind of algorithm that assigns radio-host relationships based on not just the radio source itself, but also on the other radio sources around it and how they have been paired to their own host galaxies.

    The way that our labels were generated for BXID could be improved. Our algorithms in \autoref{cha:cross-id} and \autoref{cha:rlfs} were trained on labels generated by Radio Galaxy Zoo. These labels were aggregated from multiple different labellers (usually 20) by majority vote, with the most common label for any given radio object being assigned as the true label in Radio Galaxy Zoo. This is not the only possible aggregation strategy, though. We employed the Dawid-Skene method \citep{dawid79em} ourselves in \autoref{sec:rlfs-manual-validation} to help assess the performance of our cross-identification algorithm, and this model for example may also be applied to Radio Galaxy Zoo. There are in fact aggregation strategies that work in tandem with a machine learning model to get better labels, such as \citet{raykar_learning_2010}. These methods simultaneously take into account the labels and the labellers, and can accommodate for different levels of ability in the labellers, or different levels of difficulty in the examples being labelled.

    Our RLFs could be improved. The RLF calculations in \autoref{cha:rlfs} are severely limited by the availability of redshifts. We limited our analyses to host galaxies that did have available spectroscopic redshifts in SDSS, but we could also have employed the less-reliable but considerably more prolific photometric redshifts. These are derived from regression models rather than direct observations of redshifted spectral lines, and so can be produced from photometric surveys without dedicated spectroscopy. However, without methods to handle the uncertainty introduced by photometric redshifts, the resulting RLFs would be unreliable. Decreasing the uncertainty in photometric redshifts is not the only way forward. We may also develop methods for understanding and incorporating their uncertainties into downstream calculations like those used for RLFs, for example, using probabilistic programming \citep[e.g.][]{bingham2018pyro}. This will allow these photometric redshifts to be used and tremendously increase sample sizes. Besides existing photometric redshifts, future surveys will also produce many more redshifts, both spectroscopic and photometric. This will be very important for surveys like EMU, which are both deep and wide with low redshift availability.

    The RGZ-Ex dataset (\autoref{cha:rlfs}) also lets us pose many other interesting science questions. We demonstrated in \aref{sec:rlfs-giants} that rare galaxy classes can be identified from within this dataset, including examples that have never before been identified in the literature. Our dataset may be augmented with other features and used to identify unusual objects in a similar way. Besides this, our fractional RLFs could also be extended with any number of galaxy properties. One particularly interesting property could be morphology, as other algorithms in radio astroinformatics are developed which can automatically identify morphologies \citep[e.g.][]{wu19claran}. Such a classifier could be used to segment RGZ-Ex and a fractional morphological RLF could be obtained. These morphologies may even be classes that are not easily separated, such as those found by self-organising maps \citep[e.g.][]{polsterer15pink}. Of course, there are other properties that are more easily extracted, such as optical lines and colours which could be taken from SDSS using our SDSS cross-identifications.

     While creating features for FDFs in \autoref{cha:faraday} we demonstrated that W2 distance was a sensible distance measure between FDFs. This is useful for more than just feature construction, as it implies a geometry on the space of FDFs. This distance could be used to help gain insight on the behaviours of future algorithms that work on FDFs. A particularly exciting idea is to improve QU fitting by modifying the distance function to match ours. Our features could also be used to develop other methods for FDF analysis, like outlier detection or data visualisation.

     Further research is needed to close the domain gap for FDFs. This is an interesting case study as it is such a simple case, where we know essentially all the physics behind the observations and the observations are one-dimensional. Even this is not enough, and whether through unmodelled physics (e.g. more than two screens, depolarisation) or unmodelled observational properties (e.g. radio frequency interference), simulation and observation do not fully line up. Such research is critical if we want to train machine learning algorithms on simulations in the future, and we very much want to do this to augment our limited observational training data. Similarly, the domain gap must be reduced for radio continuum observations. Our results in \autoref{cha:cross-id} show that pilot surveys like ATLAS may not contain enough complex sources to train machine learning models, and while larger surveys like FIRST exist, transferring models from a survey undertaken with one set of observing parameters (telescope, frequency, depth, resolution...) is both non-trivial and as-yet relatively unexplored.

\section{Implications for radio citizen science}
\label{sec:implications-citizen-science}

    Our results in \autoref{cha:cross-id} demonstrated that machine learning methods trained on citizen science labels perform comparably to those trained on expert labels, even when those labels are lower quality than expert labels. We applied this insight in \autoref{cha:rlfs} to obtain scientific results, using machine learning to extrapolate the labels to a larger dataset. While Radio Galaxy Zoo alone was not enough to fully label FIRST, Radio Galaxy Zoo working in tandem with machine learning was. This is a pattern that may hold true for future surveys and applications, too. Future astronomical research at-scale may leverage the idea of people working with machine learning, sometimes called \defn{human-in-the-loop} \citep[e.g.][]{holzinger_interactive_2016} learning, to pore through data-at-scale.

\section{Implications for wide-area radio surveys}
\label{sec:implications-wide-area-radio-surveys}

    As we move toward larger and larger datasets, an important question is how applicable our models will be across the sky. Our results in \autoref{cha:cross-id} showed that we can expect some generalisation, as our model trained on one patch of sky was applicable to another without a great loss of performance. Similarly, our classifier trained on part of FIRST seemed to work well on the rest of the dataset. This is promising as it implies that limited area surveys may help develop training sets that generalise to the whole sky, potentially making the process of generating training sets considerably cheaper.

    With our work in \autoref{cha:cross-id} and \ref{cha:rlfs} we demonstrated that a large set of good quality, complex data is required for training good astroinformatics algorithms. Pilot datasets like ATLAS will not work by themselves: The sources they contain are too simple and their complex sources are too few. A sensible question to ask is, could we simulate data for training purposes? We trained our classifier in \autoref{cha:faraday} on simulated data and found that it was difficult to bridge the domain gap between simulation and observation, even in a well-understood, one-dimensional case---let alone the complex three-dimensional projected morphologies we observe in imagery. Getting across this domain gap will be difficult and will necessarily be a major topic of research in the astroinformatics field in the near future.

    Tying observations, simulations, and models together are the representation of the data: features. Our results in \autoref{cha:faraday} show that judicious choice of features can outperform even complex and powerful models. This is good for two reasons. The first reason is that these features may be more easily interpreted. The meaning of the features may be understood to be representative of some physical property, or at least the relationship between physical reality and predictions may be more easily found. The second reason is that features may be selected which can generalise well to datasets beyond just the training set. In other words, features that are less overfit to the training set. This is of particular concern when developing predictive models on simulated training data, as features being less suited for real data than for simulations is one aspect that may contribute to the domain gap. Choosing good features in astronomy may be more important than in many other fields to which machine learning is applied, as while in most fields it is possible to conduct experiments, in astronomy we only have one Universe to look at. We need to make the best use we can of the limited radio sky.

    % - domain gap for training on simulated data for FDFs
    % - domain gap for simulated radio continuum data to augment surveys like ATLAS
    % - apply our cross-identification and Faraday complexity methods to EMU and POSSUM, as well as surveys like LoTSS and GLEAM-X
    % - improve our aggregation method for bxid
    % - generate alternative fractional RLFs, e.g. by ML-derived morphology
    % - incorporate photometric redshifts into calculations from rlf
    % - investigate the link between extended sources and their local environments
    % - apply other crowd aggregation strategies to RGZ
    % - use our insights into FDFs to do other interesting things, like outlier detection, data visualisation, or some new way of qu fitting

\section{Final remarks}
\label{sec:final-remarks}

    Radio astronomy faces a deluge of data, with current and upcoming surveys delivering incredible amounts of data for science use. While we can get a lot out of these data---from investigations into new physics, to finding rare and unusual objects---doing so is dependent on the development of new methods for astronomy at scale. These future methods will necessarily be computational, and so the challenge lies in encoding the abstract concepts of astronomy and astrophysics into a rigorously defined set of rules that a computer can interpret and execute en masse. This is decidedly non-trivial, and the nuance and unique skills required to do so motivates the burgeoning field of astroinformatics. By combining concepts from deep within the often disparate fields of astronomy and machine learning, we hope that we will be able to make the absolute most of the incredible new technologies and instruments that will arise in the future of radio astronomy.
    % In this thesis we have introduced two machine learning methods for working with wide-area radio surveys and applied them to real data. 

    % something something we need to encode our astronomical knowledge in a computer which does exactly what it tells us. but that's not how astro works!

% \todo{reflect on the big picture from the very start of the intro in a "final remarks" section}